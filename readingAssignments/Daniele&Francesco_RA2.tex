\documentclass[10pt]{proc}

\begin{document}

\large{\textbf{Reading Assignment 2}}\\

\large{Authors:\textbf{ \\Daniele Montesi, Francesco Staccone}}\\

\large{Paper title: \textbf{MLlib*: Fast Training of GLMs using Spark MLlib}}\\

\section{Motivation}
Running machine learning workloads in an efficient and distributed way is becoming more and more relevant because of the increasing availability of large datasets used to  
carry out highly computational demanding tasks, such as training deep neural networks.\\ In this context, the paper takes into account the increasing popularity of Spark, a unified analytics engine for big data processing developed by Apache that many companies use to extract and transform their data, but not for machine learning workloads. In particular, a small percentage of them actually use MLlib, an official built-in module for machine learning, while the rest prefer other specialized systems such as TensorFlow or XGBoost, even if this implies significant data movement overhead for dataset migrations from Spark.\\
That is due to the general belief that Spark is slow when it comes to distributed machine learning, so that the paper aims to understand the reasons for MLlib slowness figuring out if it is actually caused by  architectural barriers or by implementation issues, and propose a solution.
\section{Contributions}
Focusing on training generalized linear models (GLM) as a case study, the paper reveals that are actually implementation issues rather than fundamental barriers that prevent Spark from achieving superb performance. Specifically, the study identifies \textbf{two major bottlenecks} in the MLlib implementation of gradient descent (GD), one of the most popular optimization algorithms used for training GLMs. They are:
\begin{itemize}
    \item \textbf{Pattern of Model Update}, that is not efficient. In MLlib there is a driver node responsible for updating the (global) model, whereas the worker nodes simply compute the derivatives and send them to the driver. This is inefficient because the global model shared by the workers can only be updated once per communication step between the workers and the driver.
    \item \textbf{Pattern of Communication}, that can be improved. In MLlib while the driver is updating the model, the workers have to wait until the update is finished and the updated model is transferred back. Apparently the driver becomes a bottleneck, especially for large models.
\end{itemize}

\section{Solution}
By carefully designing and consolidating two state-of-the-art techniques in MLlib, the authors implemented a new machine learning library on Spark called MLlib*. They address the issues mentioned in the previous section by leveraging on \textbf{model averaging}, a widely adopted technique in distributed machine learning systems. The basic idea behind it is that each worker updates its local view of the model and the driver simply takes the average of the local views received from individual workers as the updated global model. In this way the global model is actually updated many times per communication step and therefore the number of communication steps towards convergence is reduced. Moreover, model averaging can be performed in a distributed manner across the workers, so that the centralized MLlib implementation based on the driver can be completely removed, reducing latency. The basic idea of this \textbf{distributed aggregation} is to partition the global model and let each
executor own one partition, so that all the executors participate in the distributed maintenance of the global model simultaneously.

\section{Strong Points}
The paper is well-explained by the authors. Among the numerous strengths, we present here three of them:
\begin{itemize}
    \item \textbf{Communication improvements} between the driver and the nodes are widely shown in the \textbf{Solution} section of this review and represent the main strenght of the paper. Thanks to the \textbf{model averaging} and \textbf{distributed aggregation}, one communication step of MLlib* corresponds to \textit{many} updates of the Global model, aiming for a faster convergence that is really evident on the benchmark comparison section of the paper.
    \item \textbf{Lower latency} between communication steps has been achieved by MLlib after a two-phases procedure: \textit{Reduce-Scatter} and \textit{AllGather}. The procedure let MLlib* achieving a great improvement if compared with the \textit{MLlib + model averaging} system. The comparisons are made visibly clear by the author thanks to the figures "Fig. 3B" and "Fig. 3C" showed in the paper. 
    \item \textbf{Exhaustiveness of the background} lets the reader understand all the steps followed by the authors. Starting from the distributed MGD description, the well-explained pseudo-codes, and the figures, it is possible to easily understand the general distributed mechanism for training an algorithm. Secondly, the authors introduces the existing distributed machine learning systems and compare different approaches underlining the similarities and the differences, as well doing accurate benchmark comparisons. For instance, the authors found out that in \textit{Petuum}, the \textit{model summation} procedure was performing slowly and decided to slightly modify it by replacing the model summation with the \textit{model averaging} (which worked the best with MLlib* as well) calling the new system \textit{Pentuum*}. The authors decided also to fairly benchmark this new system.
\end{itemize}

\section{Weak Points}
Three drawbacks of the paper are here listed and discussed:
\begin{itemize}
    \item \textbf{System may decrease performance when  regularization is not null}. In particular, in the paper is shown that when the objective function is too determined, the gap between the performances of MLlib* and its predecessor MLlib* become smaller. This actually happens with all the 4 dataset. For instance, the loss of performance for \textit{avazu} is 94.5\% of the timing for convergence, passing from 123x (obtained with L2 = 0) to 7x time (L2 = 0.1).
    \item \textbf{Poor description} of some results obtained benchmarking MLlib* over the other existing platforms. The performance of the MLlib* is pretty different on every dataset. I would have expected more details explaining why one dataset performs better than the others in the opinion of the authors. Moreover, the authors don't explain details of why in some datasets (kddb, url) MLlib does not converge.
    \item \textbf{MLlib* takes more iterations to convergence}, this is due to the larger overhead that MLlib* needs in comparison on its predecessor MLlib. This overhead is given on the one hand because of MLlib* needs to pass the entire dataset on every iteration (compared with a smaller batch that MLlib passes), on the other hand, because if model size is small,  communication steps are also less and hence the benefits achieved by the \textbf{Allreduce} procedure are less effective.
\end{itemize}
\end{document}
